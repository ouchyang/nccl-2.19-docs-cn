# SOME DESCRIPTIVE TITLE.
# Copyright (C)
# This file is distributed under the same license as the NCCL(2.19) package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NCCL(2.19) \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-27 10:46+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"

#: ../../source/usage/groups.rst:5 0b1232fa4c804392bfb170d00565b3c3
msgid "Group Calls"
msgstr ""

#: ../../source/usage/groups.rst:7 34d1f789d4ff443ba1237bfc62d6c39d
msgid ""
"Group functions (ncclGroupStart/ncclGroupEnd) can be used to merge "
"multiple calls into one. This is needed for three purposes: managing "
"multiple GPUs from one thread (to avoid deadlocks), aggregating "
"communication operations to improve performance, or merging multiple "
"send/receive point-to-point operations (see :ref:`point-to-point` "
"section). All three usages can be combined together, with one exception :"
" calls to :c:func:`ncclCommInitRank` cannot be merged with others."
msgstr ""

#: ../../source/usage/groups.rst:14 7fa66aaaadc6425491e1d77179225299
msgid "Management Of Multiple GPUs From One Thread"
msgstr ""

#: ../../source/usage/groups.rst:16 50024dda126848e6970a7cd1cc365c7c
msgid ""
"When a single thread is managing multiple devices, group semantics must "
"be used. This is because every NCCL call may have to block, waiting for "
"other threads/ranks to arrive, before effectively posting the NCCL "
"operation on the given stream. Hence, a simple loop on multiple devices "
"like shown below could block on the first call waiting for the other "
"ones:"
msgstr ""

#: ../../source/usage/groups.rst:25 167707e3688c4d669384964fd8dbb001
msgid ""
"To define that these calls are part of the same collective operation, "
"ncclGroupStart and ncclGroupEnd should be used:"
msgstr ""

#: ../../source/usage/groups.rst:35 2b28808b8f8b454ca6d8e09cf1603928
msgid ""
"This will tell NCCL to treat all calls between ncclGroupStart and "
"ncclGroupEnd as a single call to many devices."
msgstr ""

#: ../../source/usage/groups.rst:37 a203aca22b7b4ae382c189fc0656ae54
msgid ""
"Caution: When called inside a group, stream operations (like "
"ncclAllReduce) can return without having enqueued the operation on the "
"stream. Stream operations like cudaStreamSynchronize can therefore be "
"called only after ncclGroupEnd returns."
msgstr ""

#: ../../source/usage/groups.rst:41 b9bb309b7b7e4464b4926fa7e6e2b87d
msgid ""
"Group calls must also be used to create a communicator when one thread "
"manages more than one device:"
msgstr ""

#: ../../source/usage/groups.rst:53 57d83a2a67ae429f8451ff427afff552
msgid ""
"Note: Contrary to NCCL 1.x, there is no need to set the CUDA device "
"before every NCCL communication call within a group, but it is still "
"needed when calling ncclCommInitRank within a group."
msgstr ""

#: ../../source/usage/groups.rst:56 ../../source/usage/groups.rst:101
#: ../../source/usage/groups.rst:141 1550a8576110488ebabf4414d5c941fe
#: 923c6020ad9b42cd97c51bea9bebfddc 9c6c4c3dc5be43a88eb71b45ba496b72
msgid "Related links:"
msgstr ""

#: ../../source/usage/groups.rst:58 ../../source/usage/groups.rst:103
#: 401c89de9f8c4f35ba549942969b44b6 aed721f91d434f15ba53b0b9ef73d534
msgid ":c:func:`ncclGroupStart`"
msgstr ""

#: ../../source/usage/groups.rst:59 ../../source/usage/groups.rst:104
#: 01cac7fb512a4a47b4f9806a58f061f1 cba971157b8d434db0663c3c3cb59dad
msgid ":c:func:`ncclGroupEnd`"
msgstr ""

#: ../../source/usage/groups.rst:62 0302dcf3aa044750910398c64f939853
msgid "Aggregated Operations (2.2 and later)"
msgstr ""

#: ../../source/usage/groups.rst:64 5038f5fd62504faa979f869f3ba219d1
msgid ""
"The group semantics can also be used to have multiple collective "
"operations performed within a single NCCL launch. This is useful for "
"reducing the launch overhead, in other words, latency, as it only occurs "
"once for multiple operations. Init functions cannot be aggregated with "
"other init functions, nor with communication functions."
msgstr ""

#: ../../source/usage/groups.rst:68 230d144968af46769da9fba7ca4f2503
msgid ""
"Aggregation of collective operations can be done simply by having "
"multiple calls to NCCL within a ncclGroupStart / ncclGroupEnd section."
msgstr ""

#: ../../source/usage/groups.rst:71 51a24a75d81c42c9b4fe25cf7db2e5b6
msgid ""
"In the following example, we launch one broadcast and two allReduce "
"operations together as a single NCCL launch."
msgstr ""

#: ../../source/usage/groups.rst:81 75f795e297f346bf82fe9f3dd30adb79
msgid ""
"It is permitted to combine aggregation with multi-GPU launch and use "
"different communicators in a group launch as shown in the Management Of "
"Multiple GPUs From One Thread topic.  When combining multi-GPU launch and"
" aggregation, ncclGroupStart and ncclGroupEnd can be either used once or "
"at each level. The following example groups the allReduce operations from"
" different layers and on multiple CUDA devices :"
msgstr ""

#: ../../source/usage/groups.rst:98 e6fc3145201a4fa3aad9da4754a26038
msgid ""
"Note: The NCCL operation will only be started as a whole during the last "
"call to ncclGroupEnd. The ncclGroupStart and ncclGroupEnd calls within "
"the for loop are not necessary and do nothing."
msgstr ""

#: ../../source/usage/groups.rst:107 fec651eb473a472c83c872499b0b9a4d
msgid "Nonblocking Group Operation"
msgstr ""

#: ../../source/usage/groups.rst:109 be531aebbe4344e79d04f1926a925388
msgid ""
"If a communicator is marked as nonblocking through "
"ncclCommInitRankConfig, the group functions become asynchronous "
"correspondingly. In this case, if users issue multiple NCCL operations in"
" one group, returning from ncclGroupEnd() might not mean the NCCL "
"communication kernels have been issued to CUDA streams. If ncclGroupEnd()"
" returns ncclSuccess, it means NCCL kernels have been issued to streams; "
"if it returns ncclInProgress, it means NCCL kernels are being issued to "
"streams in the background. It is users' responsibility to make sure the "
"state of the communicator changes into ncclSuccess before calling related"
" CUDA calls (e.g. cudaStreamSynchronize):"
msgstr ""

#: ../../source/usage/groups.rst:143 6b06cdd5c69d44a19136804929c0ddea
msgid ":c:func:`ncclCommInitRankConfig`"
msgstr ""

#: ../../source/usage/groups.rst:144 e2edb00a0609482c90108353a1349803
msgid ":c:func:`ncclCommGetAsyncError`"
msgstr ""

