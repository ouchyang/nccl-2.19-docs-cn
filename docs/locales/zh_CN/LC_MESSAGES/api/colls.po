# SOME DESCRIPTIVE TITLE.
# Copyright (C)
# This file is distributed under the same license as the NCCL(2.19) package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NCCL(2.19)\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-27 11:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: gpt-po v1.0.11\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"

#: ../../source/api/colls.rst:3 58b04d734bc64cf29ee2d7c07d6d2dbf
msgid "Collective Communication Functions"
msgstr "集体通信功能"

#: ../../source/api/colls.rst:6 aabbe5b5501e4bae98db3370a60c1ab5
msgid "The following NCCL APIs provide some commonly used collective operations."
msgstr "以下 NCCL API 提供了一些常用的集体操作。"

#: ../../source/api/colls.rst:9 9fc5e1d09c2b44e4903202586fdfdd80
msgid "ncclAllReduce"
msgstr "ncclAllReduce"

#: ../../source/api/colls.rst:13 32a9ccfc4a384ffca36b27a93b29781e
msgid ""
"Reduce data arrays of length ``count`` in ``sendbuff`` using ``op`` operation and leaves identical copies of the result "
"on each ``recvbuff``."
msgstr "在“sendbuff”中使用“op”操作减少长度为“count”的数据数组，并在每个“recvbuff”上保留结果的相同副本。"

#: ../../source/api/colls.rst:15 ../../source/api/colls.rst:28
#: ../../source/api/colls.rst:51 4e38b374200945aa93a375a5681479a4
#: 8e2794a68d554695a229af2a552cbe7a 9434b5475f5a47059ba1d7a0d1e0b2a4
msgid "In-place operation will happen if ``sendbuff == recvbuff``."
msgstr "如果 `sendbuff == recvbuff`，将会发生原地操作。"

#: ../../source/api/colls.rst:17 72782f6b79e948f58376a44c922e1c98
msgid "Related links: :ref:`allreduce`."
msgstr "相关链接: :ref:`allreduce`."

#: ../../source/api/colls.rst:21 2fd0ff62d17244a68707c138b527b52c
msgid "ncclBroadcast"
msgstr "ncclBroadcast"

#: ../../source/api/colls.rst:25 42845033919a4891863d88fee0d06cbf
msgid ""
"Copies ``count`` elements from ``sendbuff`` on the ``root` rank to all ranks' ``recvbuff``. ``sendbuff`` is only used "
"on rank ``root`` and ignored for other ranks."
msgstr "将“sendbuff”上的“count”个元素从“root”等级复制到所有等级的“recvbuff”中。“sendbuff”仅在等级“root”上使用，在其他等级上将被忽略。"

#: ../../source/api/colls.rst:33 808cef5adbf84c9cb5f936f5f50615df
msgid "Legacy in-place version of ``ncclBroadcast`` in a similar fashion to MPI_Bcast. A call to ::"
msgstr "类似于 MPI_Bcast 的 ``ncclBroadcast`` 的就地版本。调用 ::"

#: ../../source/api/colls.rst:37 d6bfb3bd31e5450ca298316b92881307
msgid "is equivalent to ::"
msgstr "等同于 ::"

#: ../../source/api/colls.rst:41 d46de30fa35b42c9b75947b95ab82656
msgid "Related links: :ref:`broadcast`"
msgstr "相关链接：:ref:`broadcast`"

#: ../../source/api/colls.rst:44 c51e5155d4d140d8bc4b71e476a1df29
msgid "ncclReduce"
msgstr "ncclReduce"

#: ../../source/api/colls.rst:48 2ccb0e06a1724d6da4e5bf21a3cbeffa
msgid ""
"Reduce data arrays of length ``count`` in ``sendbuff`` into ``recvbuff`` on the ``root`` rank using the ``op`` "
"operation. ``recvbuff`` is only used on rank ``root`` and ignored for other ranks."
msgstr "在“root”等级上使用“op”操作，将“sendbuff”中长度为“count”的数据数组缩减到“recvbuff”中。“recvbuff”仅在“root”等级上使用，在其他等级上将被忽略。"

#: ../../source/api/colls.rst:53 349193870ca44117b0afda745eddc249
msgid "Related links: :ref:`reduce`."
msgstr "相关链接：:ref:`reduce`。"

#: ../../source/api/colls.rst:56 88bbb0d4bcab49f89ef7212586747912
msgid "ncclAllGather"
msgstr "ncclAllGather"

#: ../../source/api/colls.rst:60 772c400c2ee14694ae97f7b17bea73f3
msgid "Gather ``sendcount`` values from all GPUs into ``recvbuff``, receiving data from rank ``i`` at offset ``i*sendcount``."
msgstr "收集所有GPU中的“sendcount”值到“recvbuff”，从排名“i”处以偏移量“i*sendcount”接收数据。"

#: ../../source/api/colls.rst:62 46e36f6950fe4f06ad2f36b0bed8cb8d
msgid ""
"Note: This assumes the receive count is equal to ``nranks*sendcount``, which means that ``recvbuff`` should have a size "
"of at least ``nranks*sendcount`` elements."
msgstr "请注意：这假设接收计数等于``nranks*sendcount``，这意味着``recvbuff``的大小应至少为``nranks*sendcount``个元素。"

#: ../../source/api/colls.rst:64 ba422066aad8474b89e561115547766d
msgid "In-place operation will happen if ``sendbuff == recvbuff + rank * sendcount``."
msgstr "如果 ``sendbuff == recvbuff + rank * sendcount``，将会发生原地操作。"

#: ../../source/api/colls.rst:66 2304f7ce061d432b86cfacc06c1b565d
msgid "Related links: :ref:`allgather`, :ref:`in-place-operations`."
msgstr "相关链接：:ref:`allgather`，:ref:`in-place-operations`。"

#: ../../source/api/colls.rst:69 63a7cd9b743d4997ba2d8bde4d1f75c0
msgid "ncclReduceScatter"
msgstr "ncclReduceScatter"

#: ../../source/api/colls.rst:73 e41ed1c9eff848cb981534f6fed7de05
msgid ""
"Reduce data in ``sendbuff`` from all GPUs using the ``op`` operation and leave the reduced result scattered over the "
"devices so that the ``recvbuff`` on rank ``i`` will contain the i-th block of the result."
msgstr "使用``op``操作从所有GPU中的``sendbuff``中减少数据，并将减少的结果分散到设备上，以便在排名为``i``的``recvbuff``上包含结果的第i个块。"

#: ../../source/api/colls.rst:76 ec5db056f3d04a07adbcae8a5b641280
msgid ""
"Note:  This assumes the send count is equal to ``nranks*recvcount``, which means that ``sendbuff`` should have a size "
"of at least ``nranks*recvcount`` elements."
msgstr "请注意：这假设发送计数等于``nranks*recvcount``，这意味着``sendbuff``的大小应至少为``nranks*recvcount``个元素。"

#: ../../source/api/colls.rst:78 033370a671cd49abaf61462bb60bf3d9
msgid "In-place operation will happen if ``recvbuff == sendbuff + rank * recvcount``."
msgstr "如果 ``recvbuff == sendbuff + rank * recvcount``，则会发生原地操作。"

#: ../../source/api/colls.rst:80 9fba6960286e46988b3ef041a18b4960
msgid "Related links: :ref:`reducescatter`, :ref:`in-place-operations`."
msgstr "相关链接：:ref:`reducescatter`，:ref:`in-place-operations`。"
