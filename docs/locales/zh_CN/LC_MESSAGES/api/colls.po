# SOME DESCRIPTIVE TITLE.
# Copyright (C)
# This file is distributed under the same license as the NCCL(2.19) package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NCCL(2.19)\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-01 14:00+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: gpt-po v1.0.11\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"

#: ../../source/api/colls.rst:3 95b4e7724da54106addb093b2ed2bbba
msgid "Collective Communication Functions"
msgstr "集合通信函数"

#: ../../source/api/colls.rst:6 550d8f9d708541af961ba6444777f8d1
msgid "The following NCCL APIs provide some commonly used collective operations."
msgstr "以下NCCL API提供若干常用集合通信操作。"

#: ../../source/api/colls.rst:9 9b194377edfd44faac4adb3ecd1aa317
msgid "ncclAllReduce"
msgstr "ncclAllReduce"

#: ../../source/api/colls.rst:13 2e6de87cb7084b629e56a371ee6ad92e
msgid ""
"Reduce data arrays of length ``count`` in ``sendbuff`` using ``op`` operation and leaves identical copies of the result "
"on each ``recvbuff``."
msgstr "对长度为 ``count`` 的 ``sendbuff`` 数据数组执行 ``op`` 运算的归约操作，并在每个 ``recvbuff`` 中留存相同的运算结果副本。"

#: ../../source/api/colls.rst:15 ../../source/api/colls.rst:28
#: ../../source/api/colls.rst:51 1325e6d3a88f48c989ff6963697b45e2
#: 32c18338e1f54c28aad7a031408cefd6 394e01d253eb4fe7ada00cd0f6847470
msgid "In-place operation will happen if ``sendbuff == recvbuff``."
msgstr "若 ``sendbuff == recvbuff`` 将触发原地操作。"

#: ../../source/api/colls.rst:17 1bee73aef8174ecbbbfa693d23168f32
msgid "Related links: :ref:`allreduce`."
msgstr "相关链接：:ref:`allreduce`。"

#: ../../source/api/colls.rst:21 6f9f70b226c147908485ed49ce256ee2
msgid "ncclBroadcast"
msgstr "ncclBroadcast"

#: ../../source/api/colls.rst:25 c786e42cb6cd4a9f9d6c7d5f33ae0082
msgid ""
"Copies ``count`` elements from ``sendbuff`` on the ``root` rank to all ranks' ``recvbuff``. ``sendbuff`` is only used "
"on rank ``root`` and ignored for other ranks."
msgstr "将 `root` 进程(rank)上 `sendbuff` 中的 `count` 个元素复制到所有进程(rank)的 `recvbuff` 中。`sendbuff` 仅在 `root` 进程(rank)上使用，其他进程(rank)将忽略该参数。"

#: ../../source/api/colls.rst:33 cb98b13b1b3b4f9e9ff818a0b6f3907d
msgid "Legacy in-place version of ``ncclBroadcast`` in a similar fashion to MPI_Bcast. A call to ::"
msgstr "传统版本的原地（in-place）``ncclBroadcast``调用方式，其行为类似于MPI_Bcast。调用语法为："

#: ../../source/api/colls.rst:37 5ebd62e86acb47c3bd6741029bcd8db2
msgid "is equivalent to ::"
msgstr "等同于 ::"

#: ../../source/api/colls.rst:41 dfc9501ae71145d5ad4424a50c444d9c
msgid "Related links: :ref:`broadcast`"
msgstr "相关链接：:ref:`广播`"

#: ../../source/api/colls.rst:44 643249c89b9f461aa7847fbed398a48c
msgid "ncclReduce"
msgstr "ncclReduce"

#: ../../source/api/colls.rst:48 51b64609fbdd4de288a946f41e44c0a2
msgid ""
"Reduce data arrays of length ``count`` in ``sendbuff`` into ``recvbuff`` on the ``root`` rank using the ``op`` "
"operation. ``recvbuff`` is only used on rank ``root`` and ignored for other ranks."
msgstr "使用``op``操作将``sendbuff``中长度为``count``的数据数组归约到``root``进程(rank)的``recvbuff``中。``recvbuff``仅在``root``进程(rank)上使用，其他进程(rank)将忽略该参数。"

#: ../../source/api/colls.rst:53 5dfaa606867c4e9391040d5be7a4111f
msgid "Related links: :ref:`reduce`."
msgstr "相关链接：:ref:`归约`。"

#: ../../source/api/colls.rst:56 397324a1d0084727ab24fafef63cf8d5
msgid "ncclAllGather"
msgstr "全收集"

#: ../../source/api/colls.rst:60 547c5ac51a7e4693b907fcefd0965073
msgid "Gather ``sendcount`` values from all GPUs into ``recvbuff``, receiving data from rank ``i`` at offset ``i*sendcount``."
msgstr "从所有GPU收集``sendcount``个数值至``recvbuff``，接收来自进程(rank)``i``的数据位于偏移量``i*sendcount``处。"

#: ../../source/api/colls.rst:62 7b79049965664325ba9c0de87f5cb40f
msgid ""
"Note: This assumes the receive count is equal to ``nranks*sendcount``, which means that ``recvbuff`` should have a size "
"of at least ``nranks*sendcount`` elements."
msgstr "注意：此处假设接收计数等于``nranks*sendcount``，这意味着``recvbuff``应至少具有``nranks*sendcount``个元素的空间。"

#: ../../source/api/colls.rst:64 89ed8356ce514b3a9489b7c56d61a1ca
msgid "In-place operation will happen if ``sendbuff == recvbuff + rank * sendcount``."
msgstr "若 ``sendbuff == recvbuff + 进程(rank) * sendcount`` 成立，将触发原地操作。"

#: ../../source/api/colls.rst:66 2b7ebf5b69a14fe1b19ee32ee470bf6d
msgid "Related links: :ref:`allgather`, :ref:`in-place-operations`."
msgstr "相关链接：:ref:`全收集操作`，:ref:`原地操作`。"

#: ../../source/api/colls.rst:69 2daf51a264364b31b4c1493aaf86d56e
msgid "ncclReduceScatter"
msgstr "ncclReduceScatter"

#: ../../source/api/colls.rst:73 2737096a041c4192ac3c527097cbf7c6
msgid ""
"Reduce data in ``sendbuff`` from all GPUs using the ``op`` operation and leave the reduced result scattered over the "
"devices so that the ``recvbuff`` on rank ``i`` will contain the i-th block of the result."
msgstr "使用`op`操作从所有GPU的`sendbuff`中归约数据，并将归约结果分散存储在各设备上，使得进程(rank)`i`的`recvbuff`将包含结果的第i个数据块。"

#: ../../source/api/colls.rst:76 2ad798c3b19f4347bfdc7d9d7f7be069
msgid ""
"Note:  This assumes the send count is equal to ``nranks*recvcount``, which means that ``sendbuff`` should have a size "
"of at least ``nranks*recvcount`` elements."
msgstr "注意：此处假设发送计数等于``nranks*recvcount``，这意味着``sendbuff``应至少包含``nranks*recvcount``个元素。"

#: ../../source/api/colls.rst:78 1f354042385948419d708625e8d9ac3c
msgid "In-place operation will happen if ``recvbuff == sendbuff + rank * recvcount``."
msgstr "若满足条件`recvbuff == sendbuff + rank * recvcount`，将执行**原地操作**。"

#: ../../source/api/colls.rst:80 dd4ebc7a63cd4e519cf3bf4a92461c28
msgid "Related links: :ref:`reducescatter`, :ref:`in-place-operations`."
msgstr "相关链接：:ref:`归约散播`, :ref:`原地操作`。"
