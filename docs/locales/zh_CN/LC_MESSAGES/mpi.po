# SOME DESCRIPTIVE TITLE.
# Copyright (C)
# This file is distributed under the same license as the NCCL(2.19) package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NCCL(2.19)\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-01 14:00+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: gpt-po v1.0.11\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"

#: ../../source/mpi.rst:3 60d3384a9c5a4da78c035961a96f3758
msgid "NCCL and MPI"
msgstr "NCCL与MPI"

#: ../../source/mpi.rst:7 417f822f4daa4628b39421ddcfd9b894
msgid "API"
msgstr "收到。请提供需要翻译的NCCL API相关内容，我将严格遵循技术术语规范进行转换。"

#: ../../source/mpi.rst:9 aa781f4af7af47b9804edce614d6c58f
msgid ""
"The NCCL API and usage is similar to MPI but there are many minor differences.  The following list summarizes these "
"differences:"
msgstr "NCCL API 和使用方式与 MPI 类似，但存在许多细微差异。以下列表总结了这些区别："

#: ../../source/mpi.rst:12 17473761b4314b009a0e188ea769a410
msgid "Using multiple devices per process"
msgstr "每个进程使用多个设备"

#: ../../source/mpi.rst:13 e0cf103788f04267b37de8b9c6da9e85
msgid ""
"Similarly to the concept of MPI endpoints, NCCL does not require ranks to be mapped 1:1 to MPI ranks. A NCCL "
"communicator may have many ranks associated to a single process (hence MPI rank if used with MPI)."
msgstr "与MPI端点概念类似，NCCL不要求秩与MPI秩保持1:1映射关系。单个进程（若与MPI联用则对应单个MPI秩）可关联NCCL通信器的多个秩。"

#: ../../source/mpi.rst:16 ba2bee05f50c43ccaad7eab357414750
msgid "ReduceScatter operation"
msgstr "规约散播操作"

#: ../../source/mpi.rst:17 08d022242e284e538397d7f85dec6977
msgid ""
"The ncclReduceScatter operation is similar to the MPI_Reduce_scatter_block operation, not the MPI_Reduce_scatter "
"operation. The MPI_Reduce_scatter function is intrinsically a \"vector\" function, while MPI_Reduce_scatter_block "
"(defined later to fill the missing semantics) provides regular counts similarly to the mirror function MPI_Allgather. "
"This is an oddity of MPI which has not been fixed for legitimate retro-compatibility reasons and that NCCL does not "
"follow."
msgstr ""
"NCCL的ReduceScatter操作类似于MPI_Reduce_scatter_block操作，而非MPI_Reduce_scatter操作。MPI_Reduce_scatter函数本质上是\"向量\"函数，而MPI_Reduce_"
"scatter_block（后期为补充缺失语义而定义）则提供与镜像函数MPI_Allgather类似的规整计数方式。这是MPI因向后兼容性考量而保留的特殊设计，NCCL并未遵循此设计。"

#: ../../source/mpi.rst:20 7d9e699157c64f688ec209490c00d54e
msgid "Send and Receive counts"
msgstr "发送与接收计数"

#: ../../source/mpi.rst:21 582d48f2cd054719a5f1a8e99c041751
msgid ""
"In many collective operations, MPI allows for different send and receive counts and types, as long as "
"sendcount*sizeof(sendtype) == recvcount*sizeof(recvtype). NCCL does not allow that, defining a single count and a "
"single data-type."
msgstr ""
"在许多集合通信操作中，MPI允许使用不同的发送与接收计数及数据类型，只要满足sendcount*sizeof(sendtype) == "
"recvcount*sizeof(recvtype)即可。而NCCL不允许这种灵活性，它要求严格使用统一的计数和单一数据类型。"

#: ../../source/mpi.rst:23 32cd29d54e2944b58c5908d7234f89c9
msgid ""
"For AllGather and ReduceScatter operations, the count is equal to the per-rank size, which is the smallest size; the "
"other count being equal to nranks*count. The function prototype clearly shows which count is provided. ncclAllgather "
"has a sendcount as argument, while ncclReduceScatter has a recvcount as argument."
msgstr ""
"对于AllGather（全收集）和ReduceScatter（规约散播）操作，count参数等于单秩（per-rank）大小，即最小尺寸；另一尺寸等于nranks*"
"count。函数原型明确显示了提供的count类型：ncclAllgather使用sendcount作为参数，而ncclReduceScatter使用recvcount作为参数。"

#: ../../source/mpi.rst:25 6fd005f65dc1447cae00458387ad0a3f
msgid ""
"Note: When performing or comparing AllReduce operations using a combination of ReduceScatter and AllGather, define the "
"sendcount and recvcount as the total count divided by the number of ranks, with the correct count rounding-up, if it is "
"not a perfect multiple of the number of ranks."
msgstr "注意：当使用ReduceScatter和AllGather组合执行或比较AllReduce操作时，应将sendcount和recvcount定义为总计数除以秩数，若总计数不是秩数的整数倍，则需采用正确的向上取整计数方式。"

#: ../../source/mpi.rst:28 7546d22be6294b21b2c969363187e3b7
msgid "Other collectives and point-to-point operations"
msgstr "其他集合通信操作与点对点操作"

#: ../../source/mpi.rst:30 ecd8386eec644b2b85e8a301005bd509
msgid ""
"NCCL does not define specific verbs for sendrecv, gather, gatherv, scatter, scatterv, alltoall, alltoallv, alltoallw, "
"nor neighbor collectives. All those operations can be simply expressed using a combination of ncclSend, ncclRecv, and "
"ncclGroupStart/ncclGroupEnd, similarly to how they can be expressed with MPI_Isend, MPI_Irecv and MPI_Waitall."
msgstr ""
"NCCL并未为sendrecv、gather、gatherv、scatter、scatterv、alltoall、alltoallv、alltoallw以及邻居集合通信定义特定的操作动词。所有这些操作均可通过组合使用ncclSend、ncc"
"lRecv及ncclGroupStart/ncclGroupEnd来实现，其实现方式类似于通过MPI_Isend、MPI_Irecv和MPI_Waitall进行表达的模式。"

#: ../../source/mpi.rst:35 f6216d5d58614b33805a64be98ef6c31
msgid "In-place operations"
msgstr "原位操作"

#: ../../source/mpi.rst:36 7b2c529134cf4e898be1d346b0108a85
msgid "For more information, see :ref:`in-place-operations`."
msgstr "有关更多信息，请参阅：:ref:`原地操作`。"

#: ../../source/mpi.rst:40 06b11ab8f02e4b598f7634126d86bd6f
msgid "Using NCCL within an MPI Program"
msgstr "在MPI程序中使用NCCL"

#: ../../source/mpi.rst:42 46ed7fb940194fe3b8d4ebb84e49b4f5
msgid ""
"NCCL can be easily used in conjunction with MPI. NCCL collectives are similar to MPI collectives, therefore, creating a "
"NCCL communicator out of an MPI communicator is straightforward. It is therefore easy to use MPI for CPU-to-CPU "
"communication and NCCL for GPU-to-GPU communication."
msgstr ""
"NCCL 可便捷地与 MPI 配合使用。NCCL 集合通信操作与 MPI 集合通信操作类似，因此基于 MPI 通信器创建 NCCL 通信器十分直观。这使得利用 MPI 处理 CPU 到 CPU 通信，同时采用 NCCL 实现 GPU 到 "
"GPU 通信变得轻而易举。"

#: ../../source/mpi.rst:46 1bd087dcb2374971be971a6770d01b1f
msgid "However, some implementation details in MPI can lead to issues when using NCCL inside an MPI program."
msgstr "然而，MPI中的某些实现细节可能导致在MPI程序内部使用NCCL时出现问题。"

#: ../../source/mpi.rst:49 ffd0b76d03914312a646fb272bdffbf6
msgid "MPI Progress"
msgstr "MPI进度"

#: ../../source/mpi.rst:51 a037ec0d904a4de49e27bb1a543e752a
msgid ""
"MPI defines a notion of progress which means that MPI operations need the program to call MPI functions (potentially "
"multiple times) to make progress and eventually complete."
msgstr "MPI定义了一种称为“进展”（progress）的概念，这意味着MPI操作需要程序调用MPI函数（可能需要多次调用）以实现进展并最终完成操作。"

#: ../../source/mpi.rst:53 0d3fdfed9f5b40abb227fb061a524d19
msgid ""
"In some implementations, progress on one rank may need MPI to be called on another rank. While this is usually bad for "
"performance, it can be argued that this is a valid MPI implementation."
msgstr "在某些实现中，某个秩上的进度可能需要通过对其他秩调用MPI来实现。尽管这通常对性能不利，但可以认为这是一种有效的MPI实现方式。"

#: ../../source/mpi.rst:55 b6a802aea1864af293435b9465a4007b
msgid ""
"As a result, blocking in a NCCL collective operations, for example calling cudaStreamSynchronize, may create a deadlock "
"in some cases because not calling MPI will not make other ranks progress, hence reach the NCCL call, hence unblock the "
"NCCL operation."
msgstr "因此，在NCCL集合通信操作中进行阻塞（例如调用cudaStreamSynchronize）可能在某些情况下导致死锁，因为不调用MPI将无法使其他秩继续执行，从而无法触达NCCL调用，最终无法解除NCCL操作的阻塞状态。"

#: ../../source/mpi.rst:57 5647d7955cf84d58b2c2fddaf8e53a5a
msgid "In that case, the cudaStreamSynchronize call should be replaced by a loop like the following:"
msgstr "在这种情况下，应使用如下循环替代cudaStreamSynchronize调用："

#: ../../source/mpi.rst:70 7a0e3dad57d24f02bbd88b8e9a9137f7
msgid "Inter-GPU Communication with CUDA-aware MPI"
msgstr "GPU间通过CUDA-aware MPI进行通信"

#: ../../source/mpi.rst:72 b607c5d10f694a4990a8c791ee7990d6
msgid "Using NCCL to perform inter-GPU communication concurrently with CUDA-aware MPI may create deadlocks."
msgstr "使用NCCL与CUDA-aware MPI并发执行GPU间通信可能产生死锁。"

#: ../../source/mpi.rst:74 587fa6c604f8425b850f2d91c3e0d1d6
msgid ""
"NCCL creates inter-device dependencies, meaning that after it has been launched, a NCCL kernel will wait (and "
"potentially block the CUDA device) until all ranks in the communicator launch their NCCL kernel. CUDA-aware MPI may "
"also create such dependencies between devices depending on the MPI implementation."
msgstr "NCCL会创建设备间依赖关系，这意味着在启动后，NCCL内核将等待（并可能阻塞CUDA设备），直到通信器中的所有秩都启动其NCCL内核。根据MPI实现的不同，支持CUDA的MPI也可能在设备间创建此类依赖关系。"

#: ../../source/mpi.rst:78 35b8b2f1277d4ba18275e0bbd2e94a54
msgid ""
"Using both MPI and NCCL to perform transfers between the same sets of CUDA devices concurrently is therefore not "
"guaranteed to be safe."
msgstr "因此，无法保证同时使用MPI和NCCL在同一组CUDA设备之间执行传输操作的安全性。"
